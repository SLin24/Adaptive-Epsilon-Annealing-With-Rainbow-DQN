{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16acbe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import gym\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "BATCH_SIZE=32\n",
    "GAMMA=0.999\n",
    "BUFFER_SIZE=10000\n",
    "MIN_REPLAY_SIZE=200\n",
    "EPSILON_INITIAL=0.3\n",
    "EPSILON_FINAL=0.0001\n",
    "EPSILON_DECAY=25000\n",
    "ORIGINAL_EPSILON_DECAY=25000\n",
    "EPSILON_FRACTION=2\n",
    "BETA_INITIAL=0.4\n",
    "BETA_FINAL=1.0\n",
    "BETA_STEPS=30000\n",
    "TARGET_UPDATE=75\n",
    "LEARNING_RATE=0.0003\n",
    "LOG_INTERVAL=1000\n",
    "LOG_DIR = './logs'\n",
    "SAVE_CNT=0\n",
    "step = 0\n",
    "\n",
    "use_duel = True\n",
    "use_double = True\n",
    "use_priority = True\n",
    "use_multi_step = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6418b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        inputLayer = int(np.prod(env.observation_space.shape))\n",
    "        self.feature_layer = nn.Sequential(nn.Linear(inputLayer, 128),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(128, 128),\n",
    "                                          nn.ReLU())\n",
    "        self.value_stream = nn.Sequential(nn.Linear(128, 128),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Linear(128,1))\n",
    "        self.advantage_stream = nn.Sequential(nn.Linear(128, 128),\n",
    "                                             nn.ReLU(),\n",
    "                                             nn.Linear(128, env.action_space.n))\n",
    "        self.importance_weights = torch.FloatTensor()\n",
    "        self.net = nn.Sequential(nn.Linear(inputLayer, 64),\n",
    "                                nn.Tanh(),\n",
    "                                nn.Linear(64, env.action_space.n))\n",
    "    def forward(self, x):\n",
    "        if (use_duel):\n",
    "            features = self.feature_layer(x)\n",
    "            value = self.value_stream(features)\n",
    "            advantage = self.advantage_stream(features)\n",
    "            Q_vals = value + advantage - advantage.mean();\n",
    "            return Q_vals\n",
    "        else:\n",
    "            return self.net(x)\n",
    "    \n",
    "    def act(self, obs):\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        Q_vals = self(obs_t.unsqueeze(0))\n",
    "        index = torch.argmax(Q_vals, dim=1)[0]\n",
    "        action = index.detach().item()\n",
    "        \n",
    "        epsilon = np.interp(step, [0, EPSILON_DECAY], [EPSILON_INITIAL, EPSILON_FINAL])\n",
    "        rnd = random.random()\n",
    "        randomAction = False\n",
    "        if (rnd <= epsilon):\n",
    "            randomAction = True\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        return action, randomAction\n",
    "    \n",
    "class Buffer(object):\n",
    "    def __init__(self, size):\n",
    "        self.bufferSize = size\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        self.priorities = deque(maxlen=size)\n",
    "        \n",
    "    def add_experience(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(max(self.priorities, default=1))\n",
    "    \n",
    "    def get_weights(self, sample_probabilities):\n",
    "        weights = 1.0 / len(self.buffer) * 1.0 / sample_probabilities\n",
    "        normalized_weights = weights / max(weights)\n",
    "        return weights\n",
    "        \n",
    "        \n",
    "    def get_probabilities(self, priority_scale):\n",
    "        scaled_priorities = np.array(self.priorities) ** priority_scale\n",
    "        sample_probabilities = scaled_priorities / sum(scaled_priorities)\n",
    "        return sample_probabilities\n",
    "    \n",
    "    def set_priorities(self, indices, errors, offset=0.1):\n",
    "        for i,e, in zip(indices, errors):\n",
    "            self.priorities[i] = (float)(abs(e) + offset)\n",
    "        \n",
    "    \n",
    "    def sample(self, batch_size, priority_scale=1.0):\n",
    "        if (use_priority):\n",
    "            sample_probabilities = self.get_probabilities(priority_scale)\n",
    "            sample_indices = np.random.choice(range(len(self.buffer)), size=batch_size, replace=False, p=sample_probabilities)\n",
    "            samples = np.array(self.buffer)[sample_indices]\n",
    "            weights = self.get_weights(sample_probabilities[sample_indices])\n",
    "            return samples, weights, sample_indices\n",
    "        else:\n",
    "            weights = np.empty(len(self.buffer))\n",
    "            weights.fill(1.0)\n",
    "            indices = range(len(self.buffer))\n",
    "            return random.sample(self.buffer, batch_size), weights, indices\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf5b9335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(reward_buffer, replay_buffer, recent_reward_buffer, target_network, online_network, optimizer, step, decay, filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    online_network.load_state_dict(checkpoint['online'])\n",
    "    target_network.load_state_dict(checkpoint['target'])\n",
    "    reward_buffer = checkpoint['reward']\n",
    "    recent_reward_buffer = checkpoint['recent']\n",
    "    replay_buffer = checkpoint['replay']\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    step = checkpoint['step']\n",
    "    decay = checkpoint['decay']\n",
    "    return reward_buffer, replay_buffer, recent_reward_buffer, target_network, online_network, optimizer, step, decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85be5b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zwj7j5xs) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Average Reward</td><td>▁▅▆▇▇▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>Current Step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>EPISODE</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>EPSILON</td><td>█▇▇▇▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>EPSILON_DECAY</td><td>▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇████████████████</td></tr><tr><td>Recent Reward</td><td>▁▅▆▇▇▇▇▇▇▇██▇█▇█████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Average Reward</td><td>-85.99731</td></tr><tr><td>Current Step</td><td>162000</td></tr><tr><td>EPISODE</td><td>1862</td></tr><tr><td>EPSILON</td><td>0.0001</td></tr><tr><td>EPSILON_DECAY</td><td>124841.5625</td></tr><tr><td>Recent Reward</td><td>-72.14</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">denim-eon-138</strong> at: <a href='https://wandb.ai/slin25x/performanceViewer/runs/zwj7j5xs' target=\"_blank\">https://wandb.ai/slin25x/performanceViewer/runs/zwj7j5xs</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./logs\\wandb\\run-20230713_155854-zwj7j5xs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zwj7j5xs). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./logs\\wandb\\run-20230713_193632-okrhb4gn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/slin25x/performanceViewer/runs/okrhb4gn' target=\"_blank\">visionary-field-145</a></strong> to <a href='https://wandb.ai/slin25x/performanceViewer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/slin25x/performanceViewer' target=\"_blank\">https://wandb.ai/slin25x/performanceViewer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/slin25x/performanceViewer/runs/okrhb4gn' target=\"_blank\">https://wandb.ai/slin25x/performanceViewer/runs/okrhb4gn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S.Lin25\\AppData\\Local\\Temp\\ipykernel_7696\\1999580556.py:74: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  samples = np.array(self.buffer)[sample_indices]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMT 0\n",
      "\n",
      "STEP 0\n",
      "Avg Reward:  nan\n",
      "RECENT SIZE:  0  AVG SIZE:  0\n",
      "AMT 10\n",
      "\n",
      "STEP 1000\n",
      "Avg Reward:  -500.0\n",
      "RECENT SIZE:  2  AVG SIZE:  2\n",
      "AMT 8\n",
      "\n",
      "STEP 2000\n",
      "Avg Reward:  -500.0\n",
      "RECENT SIZE:  4  AVG SIZE:  4\n",
      "AMT 5\n",
      "\n",
      "STEP 3000\n",
      "Avg Reward:  -498.3333333333333\n",
      "RECENT SIZE:  6  AVG SIZE:  6\n",
      "AMT 9\n",
      "\n",
      "STEP 4000\n",
      "Avg Reward:  -416.8888888888889\n",
      "RECENT SIZE:  9  AVG SIZE:  9\n",
      "AMT 9\n",
      "\n",
      "STEP 5000\n",
      "Avg Reward:  -332.6\n",
      "RECENT SIZE:  15  AVG SIZE:  15\n",
      "AMT 8\n",
      "\n",
      "STEP 6000\n",
      "Avg Reward:  -267.8636363636364\n",
      "RECENT SIZE:  22  AVG SIZE:  22\n",
      "AMT 8\n",
      "\n",
      "STEP 7000\n",
      "Avg Reward:  -223.0\n",
      "RECENT SIZE:  31  AVG SIZE:  31\n",
      "AMT 3\n",
      "\n",
      "STEP 8000\n",
      "Avg Reward:  -189.61904761904762\n",
      "RECENT SIZE:  42  AVG SIZE:  42\n",
      "AMT 7\n",
      "\n",
      "STEP 9000\n",
      "Avg Reward:  -175.45098039215685\n",
      "RECENT SIZE:  50  AVG SIZE:  51\n",
      "AMT 11\n",
      "\n",
      "STEP 10000\n",
      "Avg Reward:  -162.7704918032787\n",
      "RECENT SIZE:  50  AVG SIZE:  61\n",
      "AMT 3\n",
      "\n",
      "STEP 11000\n",
      "Avg Reward:  -152.80281690140845\n",
      "RECENT SIZE:  50  AVG SIZE:  71\n",
      "AMT 4\n",
      "\n",
      "STEP 12000\n",
      "Avg Reward:  -146.77777777777777\n",
      "RECENT SIZE:  50  AVG SIZE:  81\n",
      "AMT 7\n",
      "\n",
      "STEP 13000\n",
      "Avg Reward:  -141.53846153846155\n",
      "RECENT SIZE:  50  AVG SIZE:  91\n",
      "AMT 8\n",
      "\n",
      "STEP 14000\n",
      "Avg Reward:  -137.35643564356437\n",
      "RECENT SIZE:  50  AVG SIZE:  101\n",
      "AMT 12\n",
      "\n",
      "STEP 15000\n",
      "Avg Reward:  -133.9009009009009\n",
      "RECENT SIZE:  50  AVG SIZE:  111\n",
      "AMT 4\n",
      "\n",
      "STEP 16000\n",
      "Avg Reward:  -131.88333333333333\n",
      "RECENT SIZE:  50  AVG SIZE:  120\n",
      "AMT 9\n",
      "\n",
      "STEP 17000\n",
      "Avg Reward:  -129.66923076923078\n",
      "RECENT SIZE:  50  AVG SIZE:  130\n",
      "AMT 7\n",
      "\n",
      "STEP 18000\n",
      "Avg Reward:  -127.00714285714285\n",
      "RECENT SIZE:  50  AVG SIZE:  140\n",
      "AMT 8\n",
      "\n",
      "STEP 19000\n",
      "Avg Reward:  -126.28859060402685\n",
      "RECENT SIZE:  50  AVG SIZE:  149\n",
      "AMT 2\n",
      "\n",
      "STEP 20000\n",
      "Avg Reward:  -124.30817610062893\n",
      "RECENT SIZE:  50  AVG SIZE:  159\n",
      "AMT 4\n",
      "\n",
      "STEP 21000\n",
      "Avg Reward:  -122.66863905325444\n",
      "RECENT SIZE:  50  AVG SIZE:  169\n",
      "AMT 6\n",
      "\n",
      "STEP 22000\n",
      "Avg Reward:  -121.69832402234637\n",
      "RECENT SIZE:  50  AVG SIZE:  179\n",
      "AMT 5\n",
      "\n",
      "STEP 23000\n",
      "Avg Reward:  -119.8157894736842\n",
      "RECENT SIZE:  50  AVG SIZE:  190\n",
      "AMT 4\n",
      "\n",
      "STEP 24000\n",
      "Avg Reward:  -119.35175879396985\n",
      "RECENT SIZE:  50  AVG SIZE:  199\n",
      "AMT 6\n",
      "\n",
      "STEP 25000\n",
      "Avg Reward:  -118.64593301435407\n",
      "RECENT SIZE:  50  AVG SIZE:  209\n",
      "AMT 7\n",
      "\n",
      "STEP 26000\n",
      "Avg Reward:  -118.20183486238533\n",
      "RECENT SIZE:  50  AVG SIZE:  218\n",
      "AMT 0\n",
      "\n",
      "STEP 27000\n",
      "Avg Reward:  -116.79912663755458\n",
      "RECENT SIZE:  50  AVG SIZE:  229\n",
      "AMT 4\n",
      "\n",
      "STEP 28000\n",
      "Avg Reward:  -115.56666666666666\n",
      "RECENT SIZE:  50  AVG SIZE:  240\n",
      "AMT 5\n",
      "\n",
      "STEP 29000\n",
      "Avg Reward:  -114.27091633466135\n",
      "RECENT SIZE:  50  AVG SIZE:  251\n",
      "AMT 1\n",
      "\n",
      "STEP 30000\n",
      "Avg Reward:  -113.40839694656489\n",
      "RECENT SIZE:  50  AVG SIZE:  262\n",
      "AMT 2\n",
      "\n",
      "STEP 31000\n",
      "Avg Reward:  -112.31868131868131\n",
      "RECENT SIZE:  50  AVG SIZE:  273\n",
      "AMT 0\n",
      "\n",
      "STEP 32000\n",
      "Avg Reward:  -111.15438596491228\n",
      "RECENT SIZE:  50  AVG SIZE:  285\n",
      "AMT 1\n",
      "\n",
      "STEP 33000\n",
      "Avg Reward:  -110.48648648648648\n",
      "RECENT SIZE:  50  AVG SIZE:  296\n",
      "AMT 1\n",
      "\n",
      "STEP 34000\n",
      "Avg Reward:  -109.81045751633987\n",
      "RECENT SIZE:  50  AVG SIZE:  306\n",
      "AMT 2\n",
      "\n",
      "STEP 35000\n",
      "Avg Reward:  -109.57278481012658\n",
      "RECENT SIZE:  50  AVG SIZE:  316\n",
      "AMT 0\n",
      "\n",
      "STEP 36000\n",
      "Avg Reward:  -108.70426829268293\n",
      "RECENT SIZE:  50  AVG SIZE:  328\n",
      "AMT 1\n",
      "\n",
      "STEP 37000\n",
      "Avg Reward:  -107.81764705882352\n",
      "RECENT SIZE:  50  AVG SIZE:  340\n",
      "AMT 2\n",
      "\n",
      "STEP 38000\n",
      "Avg Reward:  -107.19088319088318\n",
      "RECENT SIZE:  50  AVG SIZE:  351\n",
      "AMT 2\n",
      "\n",
      "STEP 39000\n",
      "Avg Reward:  -106.06868131868131\n",
      "RECENT SIZE:  50  AVG SIZE:  364\n",
      "AMT 1\n",
      "\n",
      "STEP 40000\n",
      "Avg Reward:  -105.36968085106383\n",
      "RECENT SIZE:  50  AVG SIZE:  376\n",
      "AMT 0\n",
      "\n",
      "STEP 41000\n",
      "Avg Reward:  -104.6417525773196\n",
      "RECENT SIZE:  50  AVG SIZE:  388\n",
      "AMT 1\n",
      "\n",
      "STEP 42000\n",
      "Avg Reward:  -103.7431421446384\n",
      "RECENT SIZE:  50  AVG SIZE:  401\n",
      "AMT 0\n",
      "\n",
      "STEP 43000\n",
      "Avg Reward:  -102.85990338164251\n",
      "RECENT SIZE:  50  AVG SIZE:  414\n",
      "AMT 0\n",
      "\n",
      "STEP 44000\n",
      "Avg Reward:  -102.64150943396227\n",
      "RECENT SIZE:  50  AVG SIZE:  424\n",
      "AMT 0\n",
      "\n",
      "STEP 45000\n",
      "Avg Reward:  -102.08944954128441\n",
      "RECENT SIZE:  50  AVG SIZE:  436\n",
      "AMT 0\n",
      "\n",
      "STEP 46000\n",
      "Avg Reward:  -101.61383928571429\n",
      "RECENT SIZE:  50  AVG SIZE:  448\n",
      "AMT 0\n",
      "\n",
      "STEP 47000\n",
      "Avg Reward:  -101.25272331154684\n",
      "RECENT SIZE:  50  AVG SIZE:  459\n",
      "AMT 0\n",
      "\n",
      "STEP 48000\n",
      "Avg Reward:  -100.60169491525424\n",
      "RECENT SIZE:  50  AVG SIZE:  472\n",
      "AMT 0\n",
      "\n",
      "STEP 49000\n",
      "Avg Reward:  -100.17148760330579\n",
      "RECENT SIZE:  50  AVG SIZE:  484\n",
      "AMT 0\n",
      "\n",
      "STEP 50000\n",
      "Avg Reward:  -99.53923541247485\n",
      "RECENT SIZE:  50  AVG SIZE:  497\n",
      "AMT 0\n",
      "\n",
      "STEP 51000\n",
      "Avg Reward:  -98.89803921568627\n",
      "RECENT SIZE:  50  AVG SIZE:  510\n",
      "AMT 0\n",
      "\n",
      "STEP 52000\n",
      "Avg Reward:  -98.17366412213741\n",
      "RECENT SIZE:  50  AVG SIZE:  524\n",
      "AMT 0\n",
      "\n",
      "STEP 53000\n",
      "Avg Reward:  -97.69087523277467\n",
      "RECENT SIZE:  50  AVG SIZE:  537\n",
      "AMT 0\n",
      "\n",
      "STEP 54000\n",
      "Avg Reward:  -96.96733212341198\n",
      "RECENT SIZE:  50  AVG SIZE:  551\n",
      "AMT 0\n",
      "\n",
      "STEP 55000\n",
      "Avg Reward:  -96.30442477876106\n",
      "RECENT SIZE:  50  AVG SIZE:  565\n",
      "AMT 0\n",
      "\n",
      "STEP 56000\n",
      "Avg Reward:  -95.71502590673575\n",
      "RECENT SIZE:  50  AVG SIZE:  579\n",
      "AMT 0\n",
      "\n",
      "STEP 57000\n",
      "Avg Reward:  -95.27918781725889\n",
      "RECENT SIZE:  50  AVG SIZE:  591\n",
      "AMT 0\n",
      "\n",
      "STEP 58000\n",
      "Avg Reward:  -95.14925373134328\n",
      "RECENT SIZE:  50  AVG SIZE:  603\n",
      "AMT 0\n",
      "\n",
      "STEP 59000\n",
      "Avg Reward:  -94.73051948051948\n",
      "RECENT SIZE:  50  AVG SIZE:  616\n",
      "AMT 0\n",
      "\n",
      "STEP 60000\n",
      "Avg Reward:  -94.62998405103669\n",
      "RECENT SIZE:  50  AVG SIZE:  627\n",
      "AMT 0\n",
      "\n",
      "STEP 61000\n",
      "Avg Reward:  -94.0811232449298\n",
      "RECENT SIZE:  50  AVG SIZE:  641\n",
      "AMT 0\n",
      "\n",
      "STEP 62000\n",
      "Avg Reward:  -93.58931297709924\n",
      "RECENT SIZE:  50  AVG SIZE:  655\n",
      "AMT 0\n",
      "\n",
      "STEP 63000\n",
      "Avg Reward:  -92.9955223880597\n",
      "RECENT SIZE:  50  AVG SIZE:  670\n",
      "AMT 0\n",
      "\n",
      "STEP 64000\n",
      "Avg Reward:  -92.67642752562226\n",
      "RECENT SIZE:  50  AVG SIZE:  683\n",
      "AMT 0\n",
      "\n",
      "STEP 65000\n",
      "Avg Reward:  -92.2051649928264\n",
      "RECENT SIZE:  50  AVG SIZE:  697\n",
      "AMT 0\n",
      "\n",
      "STEP 66000\n",
      "Avg Reward:  -91.79266572637518\n",
      "RECENT SIZE:  50  AVG SIZE:  709\n",
      "AMT 0\n",
      "\n",
      "STEP 67000\n",
      "Avg Reward:  -92.05972222222222\n",
      "RECENT SIZE:  50  AVG SIZE:  720\n",
      "AMT 0\n",
      "\n",
      "STEP 68000\n",
      "Avg Reward:  -91.78551912568307\n",
      "RECENT SIZE:  50  AVG SIZE:  732\n",
      "AMT 0\n",
      "\n",
      "STEP 69000\n",
      "Avg Reward:  -91.71639784946237\n",
      "RECENT SIZE:  50  AVG SIZE:  744\n",
      "AMT 0\n",
      "\n",
      "STEP 70000\n",
      "Avg Reward:  -91.45310435931307\n",
      "RECENT SIZE:  50  AVG SIZE:  757\n",
      "AMT 0\n",
      "\n",
      "STEP 71000\n",
      "Avg Reward:  -91.01167315175097\n",
      "RECENT SIZE:  50  AVG SIZE:  771\n",
      "AMT 0\n",
      "\n",
      "STEP 72000\n",
      "Avg Reward:  -90.90932311621967\n",
      "RECENT SIZE:  50  AVG SIZE:  783\n",
      "AMT 0\n",
      "\n",
      "STEP 73000\n",
      "Avg Reward:  -90.65201005025126\n",
      "RECENT SIZE:  50  AVG SIZE:  796\n",
      "AMT 0\n",
      "\n",
      "STEP 74000\n",
      "Avg Reward:  -90.25277435265104\n",
      "RECENT SIZE:  50  AVG SIZE:  811\n",
      "AMT 0\n",
      "\n",
      "STEP 75000\n",
      "Avg Reward:  -89.88242424242424\n",
      "RECENT SIZE:  50  AVG SIZE:  825\n",
      "AMT 0\n",
      "\n",
      "STEP 76000\n",
      "Avg Reward:  -89.54588796185935\n",
      "RECENT SIZE:  50  AVG SIZE:  839\n",
      "AMT 0\n",
      "\n",
      "STEP 77000\n",
      "Avg Reward:  -89.21688159437281\n",
      "RECENT SIZE:  50  AVG SIZE:  853\n",
      "AMT 0\n",
      "\n",
      "STEP 78000\n",
      "Avg Reward:  -88.97347174163784\n",
      "RECENT SIZE:  50  AVG SIZE:  867\n",
      "AMT 0\n",
      "\n",
      "STEP 79000\n",
      "Avg Reward:  -88.72045454545454\n",
      "RECENT SIZE:  50  AVG SIZE:  880\n",
      "AMT 0\n",
      "\n",
      "STEP 80000\n",
      "Avg Reward:  -88.43847874720358\n",
      "RECENT SIZE:  50  AVG SIZE:  894\n",
      "AMT 0\n",
      "\n",
      "STEP 81000\n",
      "Avg Reward:  -88.22160970231532\n",
      "RECENT SIZE:  50  AVG SIZE:  907\n",
      "AMT 0\n",
      "\n",
      "STEP 82000\n",
      "Avg Reward:  -87.83640303358614\n",
      "RECENT SIZE:  50  AVG SIZE:  923\n",
      "AMT 0\n",
      "\n",
      "STEP 83000\n",
      "Avg Reward:  -87.47547974413646\n",
      "RECENT SIZE:  50  AVG SIZE:  938\n",
      "AMT 0\n",
      "\n",
      "STEP 84000\n",
      "Avg Reward:  -87.16176470588235\n",
      "RECENT SIZE:  50  AVG SIZE:  952\n",
      "AMT 0\n",
      "\n",
      "STEP 85000\n",
      "Avg Reward:  -86.86349534643226\n",
      "RECENT SIZE:  50  AVG SIZE:  967\n",
      "AMT 0\n",
      "\n",
      "STEP 86000\n",
      "Avg Reward:  -86.65545361875637\n",
      "RECENT SIZE:  50  AVG SIZE:  981\n",
      "AMT 0\n",
      "\n",
      "STEP 87000\n",
      "Avg Reward:  -86.35979899497488\n",
      "RECENT SIZE:  50  AVG SIZE:  995\n",
      "AMT 0\n",
      "\n",
      "STEP 88000\n",
      "Avg Reward:  -86.13168316831683\n",
      "RECENT SIZE:  50  AVG SIZE:  1010\n",
      "AMT 0\n",
      "\n",
      "STEP 89000\n",
      "Avg Reward:  -85.8662109375\n",
      "RECENT SIZE:  50  AVG SIZE:  1024\n",
      "AMT 0\n",
      "\n",
      "STEP 90000\n",
      "Avg Reward:  -85.6319845857418\n",
      "RECENT SIZE:  50  AVG SIZE:  1038\n",
      "AMT 0\n",
      "\n",
      "STEP 91000\n",
      "Avg Reward:  -85.37986704653372\n",
      "RECENT SIZE:  50  AVG SIZE:  1053\n",
      "AMT 0\n",
      "\n",
      "STEP 92000\n",
      "Avg Reward:  -85.1067415730337\n",
      "RECENT SIZE:  50  AVG SIZE:  1068\n",
      "AMT 0\n",
      "\n",
      "STEP 93000\n",
      "Avg Reward:  -84.98889916743755\n",
      "RECENT SIZE:  50  AVG SIZE:  1081\n",
      "AMT 0\n",
      "\n",
      "STEP 94000\n",
      "Avg Reward:  -84.75456204379562\n",
      "RECENT SIZE:  50  AVG SIZE:  1096\n",
      "AMT 0\n",
      "\n",
      "STEP 95000\n",
      "Avg Reward:  -84.48604860486049\n",
      "RECENT SIZE:  50  AVG SIZE:  1111\n",
      "AMT 0\n",
      "\n",
      "STEP 96000\n",
      "Avg Reward:  -84.288\n",
      "RECENT SIZE:  50  AVG SIZE:  1125\n",
      "AMT 0\n",
      "\n",
      "STEP 97000\n",
      "Avg Reward:  -84.08070175438597\n",
      "RECENT SIZE:  50  AVG SIZE:  1140\n",
      "AMT 0\n",
      "\n",
      "STEP 98000\n",
      "Avg Reward:  -83.91421143847487\n",
      "RECENT SIZE:  50  AVG SIZE:  1154\n",
      "AMT 0\n",
      "\n",
      "STEP 99000\n",
      "Avg Reward:  -83.68862275449102\n",
      "RECENT SIZE:  50  AVG SIZE:  1169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMT 0\n",
      "\n",
      "STEP 100000\n",
      "Avg Reward:  -83.51732882502114\n",
      "RECENT SIZE:  50  AVG SIZE:  1183\n",
      "AMT 0\n",
      "\n",
      "STEP 101000\n",
      "Avg Reward:  -83.28297161936561\n",
      "RECENT SIZE:  50  AVG SIZE:  1198\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 87\u001b[0m\n\u001b[0;32m     85\u001b[0m target_online_Q_vals \u001b[38;5;241m=\u001b[39m online_network(new_observations_t)\n\u001b[0;32m     86\u001b[0m best_indices \u001b[38;5;241m=\u001b[39m target_online_Q_vals\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 87\u001b[0m targets_target_Q_vals \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_observations_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m targets_selected_Q_vals \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mtargets_target_Q_vals, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, index\u001b[38;5;241m=\u001b[39mbest_indices)\n\u001b[0;32m     89\u001b[0m targets \u001b[38;5;241m=\u001b[39m rewards_t \u001b[38;5;241m+\u001b[39m GAMMA \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m terminal_states_t) \u001b[38;5;241m*\u001b[39m targets_selected_Q_vals\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[10], line 22\u001b[0m, in \u001b[0;36mNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (use_duel):\n\u001b[0;32m     21\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_layer(x)\n\u001b[1;32m---> 22\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     advantage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvantage_stream(features)\n\u001b[0;32m     24\u001b[0m     Q_vals \u001b[38;5;241m=\u001b[39m value \u001b[38;5;241m+\u001b[39m advantage \u001b[38;5;241m-\u001b[39m advantage\u001b[38;5;241m.\u001b[39mmean();\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('Acrobot-v1')\n",
    "wandb.init(project=\"performanceViewer\", dir=LOG_DIR)\n",
    "replay_buffer = Buffer(BUFFER_SIZE)\n",
    "reward_buffer = deque()\n",
    "recent_reward_buffer = deque(maxlen=50)\n",
    "\n",
    "eps_reward = 0\n",
    "\n",
    "target_network = Network(env)\n",
    "online_network = Network(env)\n",
    "\n",
    "target_network.load_state_dict(dict(online_network.state_dict()))\n",
    "\n",
    "optimizer = torch.optim.Adam(online_network.parameters(), LEARNING_RATE)\n",
    "\n",
    "obs = env.reset()[0]\n",
    "\n",
    "for i in range(MIN_REPLAY_SIZE):\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    transition = (obs, action, reward, terminated, truncated, new_obs, False)\n",
    "    replay_buffer.add_experience(transition)\n",
    "    obs = new_obs\n",
    "    \n",
    "    if (terminated or truncated):\n",
    "        obs = env.reset()[0]\n",
    "        \n",
    "obs = env.reset()[0]\n",
    "\n",
    "while True:\n",
    "    action, randomAction = online_network.act(obs)\n",
    "    new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    transition = (obs, action, reward, terminated, truncated, new_obs, randomAction)\n",
    "    replay_buffer.add_experience(transition)\n",
    "    obs = new_obs\n",
    "    eps_reward += reward\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        obs = env.reset()[0]\n",
    "        reward_buffer.append(eps_reward)\n",
    "        recent_reward_buffer.append(eps_reward)\n",
    "        eps_reward = 0.0\n",
    "        \n",
    "    transitions, weights, indices = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "    observations = np.asarray([s[0] for s in transitions])\n",
    "    actions = np.asarray([s[1] for s in transitions])\n",
    "    rewards = np.asarray([s[2] for s in transitions])\n",
    "    terminal_states = np.asarray([s[3] for s in transitions])\n",
    "    new_observations = np.asarray([s[5] for s in transitions])\n",
    "    random_chosen = np.asarray([s[6] for s in transitions])\n",
    "    \n",
    "    amt = random_chosen.sum()\n",
    "    EPSILON_DECAY += amt / 32 * EPSILON_FRACTION\n",
    "\n",
    "    observations_t = torch.as_tensor(observations, dtype=torch.float32)\n",
    "    actions_t = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
    "    rewards_t = torch.as_tensor(rewards, dtype=torch.float32).unsqueeze(-1)\n",
    "    terminal_states_t = torch.as_tensor(terminal_states, dtype=torch.float32).unsqueeze(-1)\n",
    "    new_observations_t = torch.as_tensor(new_observations, dtype=torch.float32)\n",
    "   \n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if use_double:\n",
    "            target_online_Q_vals = online_network(new_observations_t)\n",
    "            best_indices = target_online_Q_vals.argmax(dim=1, keepdim=True)\n",
    "            targets_target_Q_vals = target_network(new_observations_t)\n",
    "            targets_selected_Q_vals = torch.gather(input=targets_target_Q_vals, dim=1, index=best_indices)\n",
    "            targets = rewards_t + GAMMA * (1 - terminal_states_t) * targets_selected_Q_vals\n",
    "            \n",
    "        else:\n",
    "            target_Q_vals = target_network(new_observations_t)\n",
    "            max_target_Q_vals = target_Q_vals.max(dim=1, keepdim=True)[0]\n",
    "\n",
    "            targets = rewards_t + GAMMA * (1 - terminal_states_t) * max_target_Q_vals\n",
    "    \n",
    "    Q_vals = online_network(observations_t)\n",
    "    action_Q_vals = torch.gather(input=Q_vals, dim=1, index=actions_t)\n",
    "    \n",
    "    beta = np.interp(step, [0, BETA_STEPS], [BETA_INITIAL, BETA_FINAL])\n",
    "    error = targets - action_Q_vals\n",
    "    loss = nn.functional.mse_loss(action_Q_vals, targets)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        weight = sum(np.multiply(weights, loss.data.numpy()))\n",
    "    if (not use_priority):\n",
    "        weight = 1\n",
    "    loss *= (weight**beta)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    replay_buffer.set_priorities(indices, error)\n",
    "    \n",
    "    if (step % TARGET_UPDATE == 0):\n",
    "        target_network.load_state_dict(online_network.state_dict())\n",
    "    if (step % LOG_INTERVAL == 0):\n",
    "        print()\n",
    "        print('STEP', step)\n",
    "        print('Avg Reward: ', np.mean(reward_buffer))\n",
    "        print(\"RECENT SIZE: \", len(recent_reward_buffer), \" AVG SIZE: \", len(reward_buffer))\n",
    "        eps = np.interp(step, [0, EPSILON_DECAY], [EPSILON_INITIAL, EPSILON_FINAL])\n",
    "        wandb.log({\"EPSILON\": eps},commit=False)\n",
    "        wandb.log({\"EPSILON_DECAY\": EPSILON_DECAY}, commit=False)\n",
    "        wandb.log({\"Recent Reward\": np.mean(recent_reward_buffer)}, commit=False)\n",
    "        wandb.log({\"Current Step\": step}, commit=False)\n",
    "        wandb.log({\"EPISODE\": len(reward_buffer)}, commit=False)\n",
    "        wandb.log({\"Average Reward\": np.mean(reward_buffer)}, commit=True)\n",
    "        \n",
    "    step+=1\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
