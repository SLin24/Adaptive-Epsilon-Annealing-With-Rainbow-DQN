{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d63efaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import gym\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import wandb\n",
    "import math\n",
    "\n",
    "BATCH_SIZE=32\n",
    "GAMMA=0.99\n",
    "BUFFER_SIZE=10000\n",
    "MIN_REPLAY_SIZE=200\n",
    "EPSILON_INITIAL=0.3\n",
    "EPSILON_FINAL=0.0001\n",
    "EPSILON_DECAY=30000\n",
    "BETA_INITIAL=0.4\n",
    "BETA_FINAL=1.0\n",
    "BETA_STEPS=30000\n",
    "TARGET_UPDATE=75\n",
    "LEARNING_RATE=0.0003\n",
    "LOG_INTERVAL=1000\n",
    "LOG_DIR = './logs'\n",
    "\n",
    "use_duel = True\n",
    "use_double = True\n",
    "use_priority = True\n",
    "use_noise = True\n",
    "use_multi_step = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8fd93fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#referencing https://github.com/Curt-Park/rainbow-is-all-you-need/blob/master/05.noisy_net.ipynb\n",
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, std_init = 0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "        \n",
    "        self.mu_weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.sigma_weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.register_buffer(\"epsilon_weight\", torch.Tensor(out_features, in_features))\n",
    "        \n",
    "        self.mu_bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.sigma_bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.register_buffer(\"epsilon_bias\", torch.Tensor(out_features))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / math.sqrt(self.in_features)\n",
    "        self.mu_weight.data.uniform_(-mu_range, mu_range)\n",
    "        self.sigma_weight.data.fill_(self.std_init / math.sqrt(self.in_features))\n",
    "        \n",
    "        self.mu_bias.data.uniform_(-mu_range, mu_range)\n",
    "        self.sigma_bias.data.fill_(self.std_init / math.sqrt(self.out_features))\n",
    "    \n",
    "    def scale_noise(self, size):\n",
    "        rnd = torch.randn(size)\n",
    "        return rnd.sign().mul(rnd.abs().sqrt())\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        epsilon_in = self.scale_noise(self.in_features)\n",
    "        epsilon_out = self.scale_noise(self.out_features)\n",
    "        \n",
    "        self.epsilon_weight.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.epsilon_bias.copy_(epsilon_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return nn.functional.linear(x, self.mu_weight + self.sigma_weight * self.epsilon_weight, self.mu_bias + self.sigma_bias * self.epsilon_bias)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cadf4905",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        inputLayer = int(np.prod(env.observation_space.shape))\n",
    "        outputLayer = int(np.prod(env.action_space.shape))\n",
    "        self.feature_layer = nn.Sequential(nn.Linear(inputLayer, 128),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(128, 128),\n",
    "                                          nn.ReLU())\n",
    "        self.value_stream = nn.Sequential(NoisyLinear(128, 128),\n",
    "                                         nn.ReLU(),\n",
    "                                         NoisyLinear(128, 1))\n",
    "        self.advantage_stream = nn.Sequential(NoisyLinear(128, 128),\n",
    "                                             nn.ReLU(),\n",
    "                                             NoisyLinear(128, env.action_space.n))\n",
    "        self.importance_weights = torch.FloatTensor()\n",
    "        self.net = nn.Sequential(nn.Linear(inputLayer, 64),\n",
    "                                nn.Tanh(),\n",
    "                                nn.Linear(64, env.action_space.n))\n",
    "    def forward(self, x):\n",
    "        if (use_duel):\n",
    "            features = self.feature_layer(x)\n",
    "            value = self.value_stream(features)\n",
    "            advantage = self.advantage_stream(features)\n",
    "            Q_vals = value + advantage - advantage.mean();\n",
    "            return Q_vals\n",
    "        else:\n",
    "            return self.net(x)\n",
    "    \n",
    "    def act(self, obs):\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        Q_vals = self(obs_t.unsqueeze(0))\n",
    "        index = torch.argmax(Q_vals, dim=1)[0]\n",
    "        action = index.detach().item()\n",
    "        \n",
    "        epsilon = np.interp(step, [0, EPSILON_DECAY], [EPSILON_INITIAL, EPSILON_FINAL])\n",
    "        rnd = random.random()\n",
    "        if (not use_noise and rnd <= epsilon):\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        self.value_stream[0].reset_noise()\n",
    "        self.value_stream[2].reset_noise()\n",
    "        self.advantage_stream[0].reset_noise()\n",
    "        self.advantage_stream[2].reset_noise()\n",
    "\n",
    "class Buffer(object):\n",
    "    def __init__(self, size):\n",
    "        self.bufferSize = size\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        self.priorities = deque(maxlen=size)\n",
    "        \n",
    "    def add_experience(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(max(self.priorities, default=1))\n",
    "    \n",
    "    def get_weights(self, sample_probabilities):\n",
    "        weights = 1.0 / len(self.buffer) * 1.0 / sample_probabilities\n",
    "        normalized_weights = weights / max(weights)\n",
    "        return weights\n",
    "        \n",
    "        \n",
    "    def get_probabilities(self, priority_scale):\n",
    "        scaled_priorities = np.array(self.priorities) ** priority_scale\n",
    "        sample_probabilities = scaled_priorities / sum(scaled_priorities)\n",
    "        return sample_probabilities\n",
    "    \n",
    "    def set_priorities(self, indices, errors, offset=0.1):\n",
    "        for i,e, in zip(indices, errors):\n",
    "            self.priorities[i] = (float)(abs(e) + offset)\n",
    "        \n",
    "    \n",
    "    def sample(self, batch_size, priority_scale=1.0):\n",
    "        if (use_priority):\n",
    "            sample_probabilities = self.get_probabilities(priority_scale)\n",
    "            sample_indices = np.random.choice(range(len(self.buffer)), size=batch_size, replace=False, p=sample_probabilities)\n",
    "            samples = np.array(self.buffer)[sample_indices]\n",
    "            weights = self.get_weights(sample_probabilities[sample_indices])\n",
    "            return samples, weights, sample_indices\n",
    "        else:\n",
    "            weights = np.empty(len(self.buffer))\n",
    "            weights.fill(1.0)\n",
    "            indices = range(len(self.buffer))\n",
    "            return random.sample(self.buffer, batch_size), weights, indices\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e389154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mslin25x\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./logs\\wandb\\run-20230713_202912-m7j9lind</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/slin25x/performanceViewer/runs/m7j9lind' target=\"_blank\">devout-salad-146</a></strong> to <a href='https://wandb.ai/slin25x/performanceViewer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/slin25x/performanceViewer' target=\"_blank\">https://wandb.ai/slin25x/performanceViewer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/slin25x/performanceViewer/runs/m7j9lind' target=\"_blank\">https://wandb.ai/slin25x/performanceViewer/runs/m7j9lind</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S.Lin25\\AppData\\Local\\Temp\\ipykernel_4624\\441561474.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  samples = np.array(self.buffer)[sample_indices]\n",
      "C:\\Users\\S.Lin25\\AppData\\Local\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\acrobot.py:281: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"Acrobot-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 0\n",
      "Avg Reward:  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S.Lin25\\AppData\\Local\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\S.Lin25\\AppData\\Local\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 1000\n",
      "Avg Reward:  -442.0\n",
      "\n",
      "STEP 2000\n",
      "Avg Reward:  -471.0\n",
      "\n",
      "STEP 3000\n",
      "Avg Reward:  -380.7142857142857\n",
      "\n",
      "STEP 4000\n",
      "Avg Reward:  -343.90909090909093\n",
      "\n",
      "STEP 5000\n",
      "Avg Reward:  -306.1875\n",
      "\n",
      "STEP 6000\n",
      "Avg Reward:  -283.5238095238095\n",
      "\n",
      "STEP 7000\n",
      "Avg Reward:  -247.89285714285714\n",
      "\n",
      "STEP 8000\n",
      "Avg Reward:  -214.41666666666666\n",
      "\n",
      "STEP 9000\n",
      "Avg Reward:  -185.64583333333334\n",
      "\n",
      "STEP 10000\n",
      "Avg Reward:  -172.59649122807016\n",
      "\n",
      "STEP 11000\n",
      "Avg Reward:  -160.5441176470588\n",
      "\n",
      "STEP 12000\n",
      "Avg Reward:  -150.0379746835443\n",
      "\n",
      "STEP 13000\n",
      "Avg Reward:  -141.63736263736263\n",
      "\n",
      "STEP 14000\n",
      "Avg Reward:  -137.6734693877551\n",
      "\n",
      "STEP 15000\n",
      "Avg Reward:  -138.60747663551402\n",
      "\n",
      "STEP 16000\n",
      "Avg Reward:  -136.68103448275863\n",
      "\n",
      "STEP 17000\n",
      "Avg Reward:  -133.70634920634922\n",
      "\n",
      "STEP 18000\n",
      "Avg Reward:  -129.231884057971\n",
      "\n",
      "STEP 19000\n",
      "Avg Reward:  -126.55405405405405\n",
      "\n",
      "STEP 20000\n",
      "Avg Reward:  -124.81761006289308\n",
      "\n",
      "STEP 21000\n",
      "Avg Reward:  -122.92307692307692\n",
      "\n",
      "STEP 22000\n",
      "Avg Reward:  -120.65555555555555\n",
      "\n",
      "STEP 23000\n",
      "Avg Reward:  -119.29319371727749\n",
      "\n",
      "STEP 24000\n",
      "Avg Reward:  -117.61386138613861\n",
      "\n",
      "STEP 25000\n",
      "Avg Reward:  -116.29577464788733\n",
      "\n",
      "STEP 26000\n",
      "Avg Reward:  -115.33183856502242\n",
      "\n",
      "STEP 27000\n",
      "Avg Reward:  -114.23504273504274\n",
      "\n",
      "STEP 28000\n",
      "Avg Reward:  -114.06172839506173\n",
      "\n",
      "STEP 29000\n",
      "Avg Reward:  -112.74117647058823\n",
      "\n",
      "STEP 30000\n",
      "Avg Reward:  -111.2621722846442\n",
      "\n",
      "STEP 31000\n",
      "Avg Reward:  -110.25539568345324\n",
      "\n",
      "STEP 32000\n",
      "Avg Reward:  -109.61245674740485\n",
      "\n",
      "STEP 33000\n",
      "Avg Reward:  -108.59800664451828\n",
      "\n",
      "STEP 34000\n",
      "Avg Reward:  -107.56549520766774\n",
      "\n",
      "STEP 35000\n",
      "Avg Reward:  -107.18575851393189\n",
      "\n",
      "STEP 36000\n",
      "Avg Reward:  -106.59580838323353\n",
      "\n",
      "STEP 37000\n",
      "Avg Reward:  -106.03768115942029\n",
      "\n",
      "STEP 38000\n",
      "Avg Reward:  -105.42415730337079\n",
      "\n",
      "STEP 39000\n",
      "Avg Reward:  -105.18528610354224\n",
      "\n",
      "STEP 40000\n",
      "Avg Reward:  -104.62962962962963\n",
      "\n",
      "STEP 41000\n",
      "Avg Reward:  -104.13589743589743\n",
      "\n",
      "STEP 42000\n",
      "Avg Reward:  -103.8025\n",
      "\n",
      "STEP 43000\n",
      "Avg Reward:  -103.06537530266344\n",
      "\n",
      "STEP 44000\n",
      "Avg Reward:  -102.28638497652582\n",
      "\n",
      "STEP 45000\n",
      "Avg Reward:  -101.68949771689498\n",
      "\n",
      "STEP 46000\n",
      "Avg Reward:  -101.39643652561247\n",
      "\n",
      "STEP 47000\n",
      "Avg Reward:  -100.95661605206074\n",
      "\n",
      "STEP 48000\n",
      "Avg Reward:  -100.35306553911205\n",
      "\n",
      "STEP 49000\n",
      "Avg Reward:  -100.17768595041322\n",
      "\n",
      "STEP 50000\n",
      "Avg Reward:  -99.77822580645162\n",
      "\n",
      "STEP 51000\n",
      "Avg Reward:  -99.56213017751479\n",
      "\n",
      "STEP 52000\n",
      "Avg Reward:  -99.29343629343629\n",
      "\n",
      "STEP 53000\n",
      "Avg Reward:  -98.91698113207548\n",
      "\n",
      "STEP 54000\n",
      "Avg Reward:  -98.45571955719558\n",
      "\n",
      "STEP 55000\n",
      "Avg Reward:  -97.98558558558558\n",
      "\n",
      "STEP 56000\n",
      "Avg Reward:  -97.56161971830986\n",
      "\n",
      "STEP 57000\n",
      "Avg Reward:  -97.30569948186529\n",
      "\n",
      "STEP 58000\n",
      "Avg Reward:  -96.94594594594595\n",
      "\n",
      "STEP 59000\n",
      "Avg Reward:  -96.52231404958678\n",
      "\n",
      "STEP 60000\n",
      "Avg Reward:  -96.19286871961103\n",
      "\n",
      "STEP 61000\n",
      "Avg Reward:  -95.82698412698413\n",
      "\n",
      "STEP 62000\n",
      "Avg Reward:  -95.40435458786936\n",
      "\n",
      "STEP 63000\n",
      "Avg Reward:  -95.04274809160306\n",
      "\n",
      "STEP 64000\n",
      "Avg Reward:  -94.7125748502994\n",
      "\n",
      "STEP 65000\n",
      "Avg Reward:  -94.36270190895742\n",
      "\n",
      "STEP 66000\n",
      "Avg Reward:  -93.96115107913668\n",
      "\n",
      "STEP 67000\n",
      "Avg Reward:  -93.74964639321075\n",
      "\n",
      "STEP 68000\n",
      "Avg Reward:  -93.49930458970793\n",
      "\n",
      "STEP 69000\n",
      "Avg Reward:  -93.20081967213115\n",
      "\n",
      "STEP 70000\n",
      "Avg Reward:  -92.90469798657718\n",
      "\n",
      "STEP 71000\n",
      "Avg Reward:  -92.60290237467018\n",
      "\n",
      "STEP 72000\n",
      "Avg Reward:  -92.31647211413748\n",
      "\n",
      "STEP 73000\n",
      "Avg Reward:  -92.08418367346938\n",
      "\n",
      "STEP 74000\n",
      "Avg Reward:  -91.83542713567839\n",
      "\n",
      "STEP 75000\n",
      "Avg Reward:  -91.77227722772277\n",
      "\n",
      "STEP 76000\n",
      "Avg Reward:  -91.52984165651644\n",
      "\n",
      "STEP 77000\n",
      "Avg Reward:  -91.3625450180072\n",
      "\n",
      "STEP 78000\n",
      "Avg Reward:  -91.0271546635183\n",
      "\n",
      "STEP 79000\n",
      "Avg Reward:  -90.83488372093024\n",
      "\n",
      "STEP 80000\n",
      "Avg Reward:  -90.70642201834862\n",
      "\n",
      "STEP 81000\n",
      "Avg Reward:  -90.4018058690745\n",
      "\n",
      "STEP 82000\n",
      "Avg Reward:  -90.20133481646273\n",
      "\n",
      "STEP 83000\n",
      "Avg Reward:  -90.10989010989012\n",
      "\n",
      "STEP 84000\n",
      "Avg Reward:  -89.88311688311688\n",
      "\n",
      "STEP 85000\n",
      "Avg Reward:  -89.63073639274279\n",
      "\n",
      "STEP 86000\n",
      "Avg Reward:  -89.50736842105263\n",
      "\n",
      "STEP 87000\n",
      "Avg Reward:  -89.30529595015577\n",
      "\n",
      "STEP 88000\n",
      "Avg Reward:  -89.10245901639344\n",
      "\n",
      "STEP 89000\n",
      "Avg Reward:  -88.99089989888776\n",
      "\n",
      "STEP 90000\n",
      "Avg Reward:  -88.85614385614386\n",
      "\n",
      "STEP 91000\n",
      "Avg Reward:  -88.62068965517241\n",
      "\n",
      "STEP 92000\n",
      "Avg Reward:  -88.49221789883268\n",
      "\n",
      "STEP 93000\n",
      "Avg Reward:  -88.32372718539865\n",
      "\n",
      "STEP 94000\n",
      "Avg Reward:  -88.1404174573055\n",
      "\n",
      "STEP 95000\n",
      "Avg Reward:  -87.92134831460675\n",
      "\n",
      "STEP 96000\n",
      "Avg Reward:  -87.81128584643848\n",
      "\n",
      "STEP 97000\n",
      "Avg Reward:  -87.58447488584476\n",
      "\n",
      "STEP 98000\n",
      "Avg Reward:  -87.42057761732852\n",
      "\n",
      "STEP 99000\n",
      "Avg Reward:  -87.27297056199822\n",
      "\n",
      "STEP 100000\n",
      "Avg Reward:  -87.16225749559082\n",
      "\n",
      "STEP 101000\n",
      "Avg Reward:  -87.07766143106457\n",
      "\n",
      "STEP 102000\n",
      "Avg Reward:  -86.83548664944014\n",
      "\n",
      "STEP 103000\n",
      "Avg Reward:  -86.6468085106383\n",
      "\n",
      "STEP 104000\n",
      "Avg Reward:  -86.46509671993272\n",
      "\n",
      "STEP 105000\n",
      "Avg Reward:  -86.33194675540766\n",
      "\n",
      "STEP 106000\n",
      "Avg Reward:  -86.22551440329218\n",
      "\n",
      "STEP 107000\n",
      "Avg Reward:  -86.04231082180635\n",
      "\n",
      "STEP 108000\n",
      "Avg Reward:  -85.91787439613526\n",
      "\n",
      "STEP 109000\n",
      "Avg Reward:  -85.70803500397773\n",
      "\n",
      "STEP 110000\n",
      "Avg Reward:  -85.60314960629921\n",
      "\n",
      "STEP 111000\n",
      "Avg Reward:  -85.40420560747664\n",
      "\n",
      "STEP 112000\n",
      "Avg Reward:  -85.18475750577367\n",
      "\n",
      "STEP 113000\n",
      "Avg Reward:  -84.96803652968036\n",
      "\n",
      "STEP 114000\n",
      "Avg Reward:  -84.76222723852521\n",
      "\n",
      "STEP 115000\n",
      "Avg Reward:  -84.58749069247952\n",
      "\n",
      "STEP 116000\n",
      "Avg Reward:  -84.38880706921944\n",
      "\n",
      "STEP 117000\n",
      "Avg Reward:  -84.19664967225054\n",
      "\n",
      "STEP 118000\n",
      "Avg Reward:  -84.12914862914863\n",
      "\n",
      "STEP 119000\n",
      "Avg Reward:  -83.955\n",
      "\n",
      "STEP 120000\n",
      "Avg Reward:  -83.8966737438075\n",
      "\n",
      "STEP 121000\n",
      "Avg Reward:  -83.8422159887798\n",
      "\n",
      "STEP 122000\n",
      "Avg Reward:  -83.7685892981237\n",
      "\n",
      "STEP 123000\n",
      "Avg Reward:  -83.65151515151516\n",
      "\n",
      "STEP 124000\n",
      "Avg Reward:  -83.51192910702113\n",
      "\n",
      "STEP 125000\n",
      "Avg Reward:  -83.32591093117409\n",
      "\n",
      "STEP 126000\n",
      "Avg Reward:  -83.21858288770053\n",
      "\n",
      "STEP 127000\n",
      "Avg Reward:  -83.07814569536424\n",
      "\n",
      "STEP 128000\n",
      "Avg Reward:  -82.93241469816273\n",
      "\n",
      "STEP 129000\n",
      "Avg Reward:  -82.89850357839948\n",
      "\n",
      "STEP 130000\n",
      "Avg Reward:  -82.74806701030928\n",
      "\n",
      "STEP 131000\n",
      "Avg Reward:  -82.59795788130185\n",
      "\n",
      "STEP 132000\n",
      "Avg Reward:  -82.45857052498418\n",
      "\n",
      "STEP 133000\n",
      "Avg Reward:  -82.30952380952381\n",
      "\n",
      "STEP 134000\n",
      "Avg Reward:  -82.19565217391305\n",
      "\n",
      "STEP 135000\n",
      "Avg Reward:  -82.0935960591133\n",
      "\n",
      "STEP 136000\n",
      "Avg Reward:  -82.04642638973732\n",
      "\n",
      "STEP 137000\n",
      "Avg Reward:  -81.94488188976378\n",
      "\n",
      "STEP 138000\n",
      "Avg Reward:  -81.85225225225226\n",
      "\n",
      "STEP 139000\n",
      "Avg Reward:  -81.74047619047619\n",
      "\n",
      "STEP 140000\n",
      "Avg Reward:  -81.63459268004722\n",
      "\n",
      "STEP 141000\n",
      "Avg Reward:  -81.50146284376828\n",
      "\n",
      "STEP 142000\n",
      "Avg Reward:  -81.3905977945444\n",
      "\n",
      "STEP 143000\n",
      "Avg Reward:  -81.26121979286536\n",
      "\n",
      "STEP 144000\n",
      "Avg Reward:  -81.16609589041096\n",
      "\n",
      "STEP 145000\n",
      "Avg Reward:  -81.08210645526614\n",
      "\n",
      "STEP 146000\n",
      "Avg Reward:  -80.9533969679955\n",
      "\n",
      "STEP 147000\n",
      "Avg Reward:  -80.87075208913649\n",
      "\n",
      "STEP 148000\n",
      "Avg Reward:  -80.8108407079646\n",
      "\n",
      "STEP 149000\n",
      "Avg Reward:  -80.79242174629324\n",
      "\n",
      "STEP 150000\n",
      "Avg Reward:  -80.67102396514161\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m         eps_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m#     transitions = replay_buffer.sample(BATCH_SIZE)\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     transitions, weights, indices \u001b[38;5;241m=\u001b[39m \u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#     (observations, actions, rewards, terminal_states, truncated_states, new_observations) = transitions\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 86\u001b[0m, in \u001b[0;36mBuffer.sample\u001b[1;34m(self, batch_size, priority_scale)\u001b[0m\n\u001b[0;32m     84\u001b[0m     sample_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer)), size\u001b[38;5;241m=\u001b[39mbatch_size, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, p\u001b[38;5;241m=\u001b[39msample_probabilities)\n\u001b[0;32m     85\u001b[0m     samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer)[sample_indices]\n\u001b[1;32m---> 86\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_probabilities\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m samples, weights, sample_indices\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[5], line 65\u001b[0m, in \u001b[0;36mBuffer.get_weights\u001b[1;34m(self, sample_probabilities)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mappend(experience)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriorities\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriorities, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, sample_probabilities):\n\u001b[0;32m     66\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m sample_probabilities\n\u001b[0;32m     67\u001b[0m     normalized_weights \u001b[38;5;241m=\u001b[39m weights \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(weights)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('Acrobot-v1')\n",
    "wandb.init(project=\"performanceViewer\", dir=LOG_DIR)\n",
    "replay_buffer = Buffer(BUFFER_SIZE)\n",
    "reward_buffer = deque()\n",
    "\n",
    "eps_reward = 0\n",
    "\n",
    "target_network = Network(env)\n",
    "online_network = Network(env)\n",
    "\n",
    "target_network.load_state_dict(dict(online_network.state_dict()))\n",
    "\n",
    "optimizer = torch.optim.Adam(online_network.parameters(), LEARNING_RATE)\n",
    "\n",
    "obs = env.reset()[0]\n",
    "\n",
    "for i in range(MIN_REPLAY_SIZE):\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    transition = (obs, action, reward, terminated, truncated, new_obs)\n",
    "    replay_buffer.add_experience(transition)\n",
    "    obs = new_obs\n",
    "    \n",
    "    if (terminated or truncated):\n",
    "        obs = env.reset()[0]\n",
    "        \n",
    "obs = env.reset()[0]\n",
    "\n",
    "for step in itertools.count():\n",
    "    action = online_network.act(obs)\n",
    "    new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    transition = (obs, action, reward, terminated, truncated, new_obs)\n",
    "    replay_buffer.add_experience(transition)\n",
    "    obs = new_obs\n",
    "    eps_reward += reward\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        obs = env.reset()[0]\n",
    "        reward_buffer.append(eps_reward)\n",
    "        eps_reward = 0.0\n",
    "    \n",
    "    transitions, weights, indices = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "    observations = np.asarray([s[0] for s in transitions])\n",
    "    actions = np.asarray([s[1] for s in transitions])\n",
    "    rewards = np.asarray([s[2] for s in transitions])\n",
    "    terminal_states = np.asarray([s[3] for s in transitions])\n",
    "    new_observations = np.asarray([s[5] for s in transitions])\n",
    "\n",
    "    observations_t = torch.as_tensor(observations, dtype=torch.float32)\n",
    "    actions_t = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
    "    rewards_t = torch.as_tensor(rewards, dtype=torch.float32).unsqueeze(-1)\n",
    "    terminal_states_t = torch.as_tensor(terminal_states, dtype=torch.float32).unsqueeze(-1)\n",
    "    new_observations_t = torch.as_tensor(new_observations, dtype=torch.float32)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if use_double:\n",
    "            target_online_Q_vals = online_network(new_observations_t)\n",
    "            best_indices = target_online_Q_vals.argmax(dim=1, keepdim=True)\n",
    "            targets_target_Q_vals = target_network(new_observations_t)\n",
    "            targets_selected_Q_vals = torch.gather(input=targets_target_Q_vals, dim=1, index=best_indices)\n",
    "            targets = rewards_t + GAMMA * (1 - terminal_states_t) * targets_selected_Q_vals\n",
    "            \n",
    "        else:\n",
    "            target_Q_vals = target_network(new_observations_t)\n",
    "            max_target_Q_vals = target_Q_vals.max(dim=1, keepdim=True)[0]\n",
    "\n",
    "            targets = rewards_t + GAMMA * (1 - terminal_states_t) * max_target_Q_vals\n",
    "    \n",
    "    Q_vals = online_network(observations_t)\n",
    "    action_Q_vals = torch.gather(input=Q_vals, dim=1, index=actions_t)\n",
    "    \n",
    "    beta = np.interp(step, [0, BETA_STEPS], [BETA_INITIAL, BETA_FINAL])\n",
    "    error = targets - action_Q_vals\n",
    "    loss = nn.functional.mse_loss(action_Q_vals, targets)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        weight = sum(np.multiply(weights, loss.data.numpy()))\n",
    "    if (not use_priority):\n",
    "        weight = 1\n",
    "    loss *= (weight**beta)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #Reset Noise\n",
    "    online_network.reset_noise()\n",
    "    target_network.reset_noise()\n",
    "    \n",
    "    replay_buffer.set_priorities(indices, error)\n",
    "    \n",
    "    if (step % TARGET_UPDATE == 0):\n",
    "        target_network.load_state_dict(online_network.state_dict())\n",
    "    if (step % LOG_INTERVAL == 0):\n",
    "        print()\n",
    "        print('STEP', step)\n",
    "        print('Avg Reward: ', np.mean(reward_buffer))\n",
    "        wandb.log({\"Current Step\": step}, commit=False)\n",
    "        wandb.log({\"Average Reward\": np.mean(reward_buffer)}, commit=True)\n",
    "        \n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
