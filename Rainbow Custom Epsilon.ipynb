{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef617edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import gym\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "BATCH_SIZE=32\n",
    "GAMMA=0.999\n",
    "BUFFER_SIZE=10000\n",
    "MIN_REPLAY_SIZE=200\n",
    "EPSILON_INITIAL=0.2\n",
    "EPSILON_FINAL=0.0001\n",
    "EPSILON_DECAY=25000\n",
    "ORIGINAL_EPSILON_DECAY=25000\n",
    "EPSILON_FRACTION=2.5\n",
    "BETA_INITIAL=0.4\n",
    "BETA_FINAL=1.0\n",
    "BETA_STEPS=30000\n",
    "TARGET_UPDATE=75\n",
    "LEARNING_RATE=0.0003\n",
    "LOG_INTERVAL=1000\n",
    "LOG_DIR = './logs'\n",
    "SAVE_DIRS = ['./chkpt1', './chkpt2', './chkpt3', './chckpt4']\n",
    "SAVE_FREQUENCY = 50\n",
    "SAVE_CNT=0\n",
    "step = 0\n",
    "\n",
    "use_duel = True\n",
    "use_double = True\n",
    "use_priority = True\n",
    "use_multi_step = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e057004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        inputLayer = int(np.prod(env.observation_space.shape))\n",
    "        self.feature_layer = nn.Sequential(nn.Linear(inputLayer, 128),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(128, 128),\n",
    "                                          nn.ReLU())\n",
    "        self.value_stream = nn.Sequential(nn.Linear(128, 128),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Linear(128,1))\n",
    "        self.advantage_stream = nn.Sequential(nn.Linear(128, 128),\n",
    "                                             nn.ReLU(),\n",
    "                                             nn.Linear(128, 2))\n",
    "        self.importance_weights = torch.FloatTensor()\n",
    "        self.net = nn.Sequential(nn.Linear(inputLayer, 64),\n",
    "                                nn.Tanh(),\n",
    "                                nn.Linear(64, env.action_space.n))\n",
    "    def forward(self, x):\n",
    "        if (use_duel):\n",
    "            features = self.feature_layer(x)\n",
    "            value = self.value_stream(features)\n",
    "            advantage = self.advantage_stream(features)\n",
    "            Q_vals = value + advantage - advantage.mean();\n",
    "            return Q_vals\n",
    "        else:\n",
    "            return self.net(x)\n",
    "    \n",
    "    def act(self, obs):\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        Q_vals = self(obs_t.unsqueeze(0))\n",
    "        index = torch.argmax(Q_vals, dim=1)[0]\n",
    "        action = index.detach().item()\n",
    "        \n",
    "        epsilon = np.interp(step, [0, EPSILON_DECAY], [EPSILON_INITIAL, EPSILON_FINAL])\n",
    "        rnd = random.random()\n",
    "        randomAction = False\n",
    "        if (rnd <= epsilon):\n",
    "            randomAction = True\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        return action, randomAction\n",
    "    \n",
    "class Buffer(object):\n",
    "    def __init__(self, size):\n",
    "        self.bufferSize = size\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        self.priorities = deque(maxlen=size)\n",
    "        \n",
    "    def add_experience(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(max(self.priorities, default=1))\n",
    "    \n",
    "    def get_weights(self, sample_probabilities):\n",
    "        weights = 1.0 / len(self.buffer) * 1.0 / sample_probabilities\n",
    "        normalized_weights = weights / max(weights)\n",
    "        return weights\n",
    "        \n",
    "        \n",
    "    def get_probabilities(self, priority_scale):\n",
    "        scaled_priorities = np.array(self.priorities) ** priority_scale\n",
    "        sample_probabilities = scaled_priorities / sum(scaled_priorities)\n",
    "        return sample_probabilities\n",
    "    \n",
    "    def set_priorities(self, indices, errors, offset=0.1):\n",
    "        for i,e, in zip(indices, errors):\n",
    "            self.priorities[i] = (float)(abs(e) + offset)\n",
    "        \n",
    "    \n",
    "    def sample(self, batch_size, priority_scale=1.0):\n",
    "        if (use_priority):\n",
    "            sample_probabilities = self.get_probabilities(priority_scale)\n",
    "            sample_indices = np.random.choice(range(len(self.buffer)), size=batch_size, replace=False, p=sample_probabilities)\n",
    "            samples = np.array(self.buffer)[sample_indices]\n",
    "            weights = self.get_weights(sample_probabilities[sample_indices])\n",
    "            return samples, weights, sample_indices\n",
    "        else:\n",
    "            weights = np.empty(len(self.buffer))\n",
    "            weights.fill(1.0)\n",
    "            indices = range(len(self.buffer))\n",
    "            return random.sample(self.buffer, batch_size), weights, indices\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(reward_buffer, replay_buffer, recent_reward_buffer, target_network, online_network, optimizer, step, decay, filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    online_network.load_state_dict(checkpoint['online'])\n",
    "    target_network.load_state_dict(checkpoint['target'])\n",
    "    reward_buffer = checkpoint['reward']\n",
    "    recent_reward_buffer = checkpoint['recent']\n",
    "    replay_buffer = checkpoint['replay']\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    step = checkpoint['step']\n",
    "    decay = checkpoint['decay']\n",
    "    return reward_buffer, replay_buffer, recent_reward_buffer, target_network, online_network, optimizer, step, decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc2507",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0', render_mode = \"rgb_array\")\n",
    "wandb.init(project=\"performanceViewer\", dir=LOG_DIR)\n",
    "replay_buffer = Buffer(BUFFER_SIZE)\n",
    "reward_buffer = deque()\n",
    "recent_reward_buffer = deque(maxlen=50)\n",
    "\n",
    "eps_reward = 0\n",
    "\n",
    "target_network = Network(env)\n",
    "online_network = Network(env)\n",
    "\n",
    "target_network.load_state_dict(dict(online_network.state_dict()))\n",
    "\n",
    "optimizer = torch.optim.Adam(online_network.parameters(), LEARNING_RATE)\n",
    "\n",
    "obs = env.reset()[0]\n",
    "\n",
    "for i in range(MIN_REPLAY_SIZE):\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    transition = (obs, action, reward, terminated, truncated, new_obs, False)\n",
    "    replay_buffer.add_experience(transition)\n",
    "    obs = new_obs\n",
    "    \n",
    "    if (terminated or truncated):\n",
    "        obs = env.reset()[0]\n",
    "        \n",
    "obs = env.reset()[0]\n",
    "\n",
    "while True:\n",
    "    action, randomAction = online_network.act(obs)\n",
    "    new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    transition = (obs, action, reward, terminated, truncated, new_obs, randomAction)\n",
    "    replay_buffer.add_experience(transition)\n",
    "    obs = new_obs\n",
    "    eps_reward += reward\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        obs = env.reset()[0]\n",
    "        reward_buffer.append(eps_reward)\n",
    "        recent_reward_buffer.append(eps_reward)\n",
    "        eps_reward = 0.0\n",
    "        if (SAVE_CNT < 4 and len(reward_buffer)>0 and len(reward_buffer) % SAVE_FREQUENCY == 0):\n",
    "            state = {'online': online_network.state_dict(), \n",
    "                    'target': target_network.state_dict(), \n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'reward': reward_buffer,\n",
    "                    'replay': replay_buffer,\n",
    "                    'recent': recent_reward_buffer,\n",
    "                     'step': step,\n",
    "                     'decay': EPSILON_DECAY\n",
    "                   }\n",
    "            torch.save(state, SAVE_DIRS[SAVE_CNT])\n",
    "            SAVE_CNT+=1\n",
    "    \n",
    "    transitions, weights, indices = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "    observations = np.asarray([s[0] for s in transitions])\n",
    "    actions = np.asarray([s[1] for s in transitions])\n",
    "    rewards = np.asarray([s[2] for s in transitions])\n",
    "    terminal_states = np.asarray([s[3] for s in transitions])\n",
    "    new_observations = np.asarray([s[5] for s in transitions])\n",
    "    random_chosen = np.asarray([s[6] for s in transitions])\n",
    "    \n",
    "    amt = random_chosen.sum()\n",
    "    EPSILON_DECAY += amt / 32 * EPSILON_FRACTION\n",
    "\n",
    "    observations_t = torch.as_tensor(observations, dtype=torch.float32)\n",
    "    actions_t = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
    "    rewards_t = torch.as_tensor(rewards, dtype=torch.float32).unsqueeze(-1)\n",
    "    terminal_states_t = torch.as_tensor(terminal_states, dtype=torch.float32).unsqueeze(-1)\n",
    "    new_observations_t = torch.as_tensor(new_observations, dtype=torch.float32)\n",
    "   \n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if use_double:\n",
    "            target_online_Q_vals = online_network(new_observations_t)\n",
    "            best_indices = target_online_Q_vals.argmax(dim=1, keepdim=True)\n",
    "            targets_target_Q_vals = target_network(new_observations_t)\n",
    "            targets_selected_Q_vals = torch.gather(input=targets_target_Q_vals, dim=1, index=best_indices)\n",
    "            targets = rewards_t + GAMMA * (1 - terminal_states_t) * targets_selected_Q_vals\n",
    "            \n",
    "        else:\n",
    "            target_Q_vals = target_network(new_observations_t)\n",
    "            max_target_Q_vals = target_Q_vals.max(dim=1, keepdim=True)[0]\n",
    "\n",
    "            targets = rewards_t + GAMMA * (1 - terminal_states_t) * max_target_Q_vals\n",
    "    \n",
    "    Q_vals = online_network(observations_t)\n",
    "    action_Q_vals = torch.gather(input=Q_vals, dim=1, index=actions_t)\n",
    "    \n",
    "    beta = np.interp(step, [0, BETA_STEPS], [BETA_INITIAL, BETA_FINAL])\n",
    "    error = targets - action_Q_vals\n",
    "    loss = nn.functional.mse_loss(action_Q_vals, targets)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        weight = sum(np.multiply(weights, loss.data.numpy()))\n",
    "    if (not use_priority):\n",
    "        weight = 1\n",
    "    loss *= (weight**beta)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    replay_buffer.set_priorities(indices, error)\n",
    "    \n",
    "    if (step % TARGET_UPDATE == 0):\n",
    "        target_network.load_state_dict(online_network.state_dict())\n",
    "    if (step % LOG_INTERVAL == 0):\n",
    "        print()\n",
    "        print('STEP', step)\n",
    "        print('Avg Reward: ', np.mean(reward_buffer))\n",
    "        print(\"RECENT SIZE: \", len(recent_reward_buffer), \" AVG SIZE: \", len(reward_buffer))\n",
    "        eps = np.interp(step, [0, EPSILON_DECAY], [EPSILON_INITIAL, EPSILON_FINAL])\n",
    "        wandb.log({\"EPSILON\": eps},commit=False)\n",
    "        wandb.log({\"EPSILON_DECAY\": EPSILON_DECAY}, commit=False)\n",
    "        wandb.log({\"Recent Reward\": np.mean(recent_reward_buffer)}, commit=False)\n",
    "        wandb.log({\"Current Step\": step}, commit=False)\n",
    "        wandb.log({\"EPISODE\": len(reward_buffer)}, commit=False)\n",
    "        wandb.log({\"Average Reward\": np.mean(reward_buffer)}, commit=True)\n",
    "#         summary_writer.add_scalar(\"Average Reward\", np.mean(reward_buffer), global_step = step)\n",
    "        \n",
    "    step+=1\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21986899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
